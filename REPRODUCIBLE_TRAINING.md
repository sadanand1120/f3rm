# Reproducible Training in F3RM

F3RM now includes comprehensive seeding support for reproducible training. This guide explains how to use these features and what they do.

## Overview

**TLDR**: F3RM now automatically sets up comprehensive seeding for reproducible training. By default, training uses seed 42 and is fully reproducible across runs.

## What's Been Added

### 1. Comprehensive Seeding Support (`f3rm/utils/seeding.py`)

- **PyTorch seeding**: `torch.manual_seed()`, `torch.cuda.manual_seed_all()`
- **NumPy seeding**: `np.random.seed()`
- **Python seeding**: `random.seed()`
- **Environment variables**: `PYTHONHASHSEED`, `CUBLAS_WORKSPACE_CONFIG`, `TCNN_SEED`
- **CUDA deterministic settings**: `torch.backends.cudnn.deterministic = True`
- **Deterministic algorithms**: `torch.use_deterministic_algorithms(True)`

### 2. Custom F3RM Trainer (`f3rm/trainer.py`)

- Extends nerfstudio's trainer with seeding capabilities
- Handles multi-GPU distributed seeding properly
- Provides detailed logging and validation
- Configurable seeding behavior

### 3. Updated F3RM Configuration (`f3rm/f3rm_config.py`)

- Now uses `F3RMTrainerConfig` instead of `TrainerConfig`
- Seeding enabled by default
- Fully configurable seeding parameters

## How It Works

### Default Behavior

When you run `ns-train f3rm`, the system now:

1. **Sets up comprehensive seeding** before any training begins
2. **Uses seed 42 by default** (from nerfstudio's `MachineConfig`)
3. **Enables deterministic algorithms** for maximum reproducibility
4. **Configures CUDA** for deterministic behavior
5. **Handles multi-GPU** setups with per-process seeding
6. **Validates the setup** and warns about potential issues

### Single GPU Training

```bash
# Default training - now fully reproducible
ns-train f3rm --data your_data

# With custom seed
ns-train f3rm --data your_data --machine.seed 123
```

### Multi-GPU Training

```bash
# Multi-GPU training with seeding
export CUDA_VISIBLE_DEVICES='0,1,2,3'
ns-train f3rm --data your_data --machine.num-devices 4

# Each GPU gets a unique seed: base_seed + rank
# GPU 0: seed 42, GPU 1: seed 43, GPU 2: seed 44, GPU 3: seed 45
```

## Configuration Options

You can customize seeding behavior by modifying `f3rm_config.py` or using command-line arguments:

### Available Parameters

```python
# In f3rm_config.py
config=F3RMTrainerConfig(
    # ... other config ...
    
    # Seeding configuration
    enable_comprehensive_seeding=True,      # Master switch for seeding
    seed_deterministic_algorithms=True,     # Use deterministic algorithms
    seed_warn_only=False,                  # Only warn about non-deterministic ops
    seed_cublas_workspace=True,            # Configure cuBLAS for determinism
    print_seed_info=True,                  # Print detailed seeding info
)
```

### Command-Line Usage

```bash
# Disable seeding (not recommended for reproducibility)
ns-train f3rm --data your_data --enable-comprehensive-seeding False

# Use a different seed
ns-train f3rm --data your_data --machine.seed 12345

# Enable warn-only mode (if you encounter deterministic algorithm issues)
ns-train f3rm --data your_data --seed-warn-only True
```

## Performance Considerations

### Impact on Training Speed

**Deterministic algorithms may slow down training by 10-30%**, but ensure perfect reproducibility.

If you need maximum speed and can accept slightly less reproducibility:

```python
# In f3rm_config.py
seed_deterministic_algorithms=False,    # Faster but less deterministic
seed_warn_only=True,                   # Don't error on non-deterministic ops
```

### Memory Usage

Deterministic algorithms may use slightly more GPU memory. If you encounter OOM errors:

1. Reduce batch size: `--pipeline.datamanager.train-num-rays-per-batch 4096`
2. Disable cuBLAS workspace: `seed_cublas_workspace=False`
3. Use warn-only mode: `seed_warn_only=True`

## Multi-GPU Considerations

### How Multi-GPU Seeding Works

1. **Base seed** is set via `--machine.seed` (default: 42)
2. **Each GPU process** gets `base_seed + rank`
   - GPU 0: seed 42
   - GPU 1: seed 43
   - GPU 2: seed 44
   - etc.
3. **Data loading** is seeded consistently across processes
4. **Model initialization** is deterministic per process

### Why This Approach?

- **Reproducible across runs**: Same seed always produces same results
- **Different per GPU**: Prevents GPUs from doing identical work
- **Consistent data splits**: Training/validation splits are identical across runs
- **Deterministic gradients**: All-reduce operations are deterministic

## Troubleshooting

### Common Issues

#### 1. "Function X does not have a deterministic implementation"

```bash
# Solution 1: Use warn-only mode
ns-train f3rm --data your_data --seed-warn-only True

# Solution 2: Disable deterministic algorithms
# Edit f3rm_config.py: seed_deterministic_algorithms=False
```

#### 2. Out of Memory (OOM) Errors

```bash
# Reduce batch size
ns-train f3rm --data your_data --pipeline.datamanager.train-num-rays-per-batch 4096

# Or disable cuBLAS workspace configuration
# Edit f3rm_config.py: seed_cublas_workspace=False
```

#### 3. Training Hangs or Crashes in Multi-GPU

```bash
# Use warn-only mode for multi-GPU
ns-train f3rm --data your_data --seed-warn-only True
# Or edit f3rm_config.py: seed_warn_only=True
```

### Validation

The system automatically validates your seeding setup. Look for these messages:

```
ðŸŒ± Setting comprehensive random seed: 42
âœ… Comprehensive seeding completed with seed: 42
ðŸ”’ Deterministic algorithms enabled
âœ… Reproducibility setup looks good!
```

## Examples

### Basic Reproducible Training

```bash
# These two runs will produce IDENTICAL results
ns-train f3rm --data datasets/scene1 --machine.seed 42
ns-train f3rm --data datasets/scene1 --machine.seed 42
```

### Multi-GPU Reproducible Training

```bash
export CUDA_VISIBLE_DEVICES='0,1,2,3'
ns-train f3rm --data datasets/scene1 --machine.num-devices 4 --machine.seed 123

# This will produce identical results to the above
export CUDA_VISIBLE_DEVICES='0,1,2,3'
ns-train f3rm --data datasets/scene1 --machine.num-devices 4 --machine.seed 123
```

### High-Performance Mode (Less Deterministic)

```python
# Edit f3rm_config.py for maximum speed
config=F3RMTrainerConfig(
    # ... other config ...
    seed_deterministic_algorithms=False,  # Faster training
    seed_warn_only=True,                 # Don't crash on non-deterministic ops
    seed_cublas_workspace=False,         # Less memory usage
)
```

## Verifying Reproducibility

To verify your setup is working:

1. **Run training twice with same seed**:
   ```bash
   ns-train f3rm --data your_data --machine.seed 42 --max-num-iterations 1000
   ns-train f3rm --data your_data --machine.seed 42 --max-num-iterations 1000
   ```

2. **Compare loss curves**: They should be identical
3. **Compare final metrics**: PSNR, SSIM should match exactly
4. **Compare model outputs**: Rendered images should be pixel-perfect matches

## Technical Details

### What Gets Seeded

- **Model weights initialization**: Hash grid, MLP weights, etc.
- **Data loading**: Image sampling, ray generation
- **Optimizer state**: Adam momentum, variance estimates
- **Feature extraction**: CLIP/DINO feature computation (if not cached)
- **Training loop**: Step ordering, batch sampling

### Environment Variables Set

- `PYTHONHASHSEED`: Controls Python's hash() function
- `CUBLAS_WORKSPACE_CONFIG`: cuBLAS deterministic workspace size
- `TCNN_SEED`: Tiny CUDA NN hash grid initialization

### Multi-GPU Synchronization

- Each process gets unique seed: `base_seed + rank`
- Data loading is synchronized across processes
- Gradient all-reduce operations are deterministic
- Model checkpoints are identical across runs

## Migration from Old F3RM

**No changes needed!** The new seeding is backward compatible:

- Old configs still work (with default seeding enabled)
- All existing command-line arguments work
- Training behavior is identical (just now reproducible)
- Performance impact is minimal with default settings

If you were previously setting seeds manually, you can remove that code as it's now handled automatically.

## Best Practices

1. **Always specify a seed** for important experiments:
   ```bash
   ns-train f3rm --data your_data --machine.seed 12345
   ```

2. **Document your seed** in your experiment logs
3. **Use the same seed** for comparison experiments
4. **Test reproducibility** on smaller experiments first
5. **Consider performance trade-offs** based on your needs

## FAQ

**Q: Will this make my training slower?**
A: Typically 10-30% slower with full deterministic mode, but you can tune the settings.

**Q: Does this work with all F3RM features?**
A: Yes, including CLIP, DINO, DINOCLIP features and multi-GPU training.

**Q: Can I disable seeding?**
A: Yes, set `enable_comprehensive_seeding=False`, but this is not recommended.

**Q: What if I get deterministic algorithm errors?**
A: Set `seed_warn_only=True` or `seed_deterministic_algorithms=False`.

**Q: Does this affect feature extraction?**
A: Only if features aren't cached. Cached features are deterministic by design.

**Q: How do I know if it's working?**
A: Run the same training twice - metrics should be identical. 